# 🚢 Titanic Survival Prediction  

## 📌 Overview  
The Titanic dataset is one of the most famous datasets in the data science community.  
The goal of this project is to predict whether a passenger survived the Titanic disaster based on features like **Age, Gender, Passenger Class, Fare, and more**.  

This project demonstrates **data cleaning, feature engineering, visualization, and predictive modeling** using machine learning.  

---

## 🎯 Objective  
- Build a machine learning model that predicts whether a passenger survived the Titanic tragedy.  
- Analyze passenger characteristics (age, gender, ticket class, etc.) and their impact on survival rate.  
- Compare different ML models to identify the most accurate approach.  

---

## ✨ Key Points  
- Dataset provided by **Kaggle Titanic Challenge** 🏆.  
- Features include **PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked**.  
- Target variable → **Survived (0 = No, 1 = Yes)**.  
- Applied **EDA (Exploratory Data Analysis)**, **Data Cleaning**, and **Feature Engineering**.  

---

## 🔑 Key Insights  
📍 Women and children had a much higher survival rate.  
📍 Passengers in **1st class** survived more compared to 2nd & 3rd class.  
📍 High ticket fares correlated with higher survival chances.  
📍 The port of embarkation also influenced survival probability.  

---

## 🛠️ Tools & Libraries Used  
- **Python 🐍**  
- **Pandas & NumPy** → Data cleaning & manipulation  
- **Matplotlib & Seaborn** → Data visualization  
- **Scikit-learn** → ML models & evaluation  
- **Jupyter Notebook** → Development environment  

---

## 🤖 Machine Learning Models Tried  
- Logistic Regression  
- K-Nearest Neighbors (KNN)  
- Decision Tree Classifier  
- Random Forest Classifier  
- Support Vector Machine (SVM)  
- Gradient Boosting  

---

## 📊 Outcomes  
✅ Achieved **good accuracy** with ensemble models like **Random Forest & Gradient Boosting**.  
✅ Logistic Regression performed strongly as a baseline model.  
✅ KNN and SVM showed decent results but required more parameter tuning.  
✅ Feature engineering (like creating “FamilySize” and encoding “Cabin/Embarked”) improved accuracy.  

---

## 🚀 Future Scope  
- Hyperparameter tuning using **GridSearchCV / RandomizedSearchCV**.  
- Feature engineering with **Name titles (Mr, Miss, Mrs, etc.)**.  
- Deployment as a **Streamlit web app** for live predictions.  
- Model explainability with **SHAP / LIME** for interpretability.  

---

## 🌟 Conclusion  
This project shows how **machine learning** can be applied to real-world historical data.  
- ✅ Women and children had the highest survival rates.  
- ✅ **Random Forest & Gradient Boosting** gave the best predictive accuracy.  
- ✅ The project strengthened understanding of **EDA, feature engineering, and classification models**.  

---

## 📂 Project Structure  

